{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 202)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 201)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   ...     var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266   ...      4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338   ...      7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155   ...      2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250   ...      4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n",
       "1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n",
       "2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n",
       "3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n",
       "4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n",
       "\n",
       "     var_7   var_8   ...     var_190  var_191  var_192  var_193  var_194  \\\n",
       "0  18.2675  2.1337   ...     -2.1556  11.8495  -1.4300   2.4508  13.7112   \n",
       "1  18.6316 -4.4131   ...     10.6165   8.8349   0.9403  10.1282  15.5765   \n",
       "2  20.2537  1.5233   ...     -0.7484  10.9935   1.9803   2.1800  12.9813   \n",
       "3  20.5660  3.3755   ...      9.5702   9.0766   1.6580   3.5813  15.1874   \n",
       "4  10.6048  2.9890   ...      4.2259   9.1723   1.2835   3.3778  19.5542   \n",
       "\n",
       "   var_195  var_196  var_197  var_198  var_199  \n",
       "0   2.4669   4.3654  10.7200  15.4722  -8.7197  \n",
       "1   0.4773  -1.4852   9.8714  19.1293 -20.9760  \n",
       "2   2.1281  -7.1086   7.0618  19.8956 -23.1794  \n",
       "3   3.1656   3.9567   9.2295  13.0168  -4.2108  \n",
       "4  -0.2860  -5.1612   7.2882  13.9260  -9.1846  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targettotrain = train['target']\n",
    "featurestotrain = train.drop(['ID_code', 'target'], axis=1)\n",
    "featurestotest = test.drop(['ID_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(featurestotrain, targettotrain, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "featurestotest = sc.transform(featurestotest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.constraints import maxnorm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 85,551\n",
      "Trainable params: 85,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=200, input_dim=200, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(0.005), kernel_constraint=maxnorm(5)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=150, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(0.005),kernel_constraint=maxnorm(5)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=100, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(0.005),kernel_constraint=maxnorm(5)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/125\n",
      "160000/160000 [==============================] - 17s 109us/step - loss: 1.5284 - acc: 0.8184 - val_loss: 1.2434 - val_acc: 0.8996\n",
      "Epoch 2/125\n",
      "160000/160000 [==============================] - 11s 71us/step - loss: 1.1077 - acc: 0.8995 - val_loss: 0.9835 - val_acc: 0.8996\n",
      "Epoch 3/125\n",
      "160000/160000 [==============================] - 12s 76us/step - loss: 0.8964 - acc: 0.8995 - val_loss: 0.7953 - val_acc: 0.8996\n",
      "Epoch 4/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.7393 - acc: 0.8995 - val_loss: 0.6692 - val_acc: 0.8996\n",
      "Epoch 5/125\n",
      "160000/160000 [==============================] - 11s 71us/step - loss: 0.6233 - acc: 0.8995 - val_loss: 0.5719 - val_acc: 0.8996\n",
      "Epoch 6/125\n",
      "160000/160000 [==============================] - 12s 74us/step - loss: 0.5373 - acc: 0.8995 - val_loss: 0.4996 - val_acc: 0.8996\n",
      "Epoch 7/125\n",
      "160000/160000 [==============================] - 12s 73us/step - loss: 0.4730 - acc: 0.8995 - val_loss: 0.4456 - val_acc: 0.8996\n",
      "Epoch 8/125\n",
      "160000/160000 [==============================] - 11s 71us/step - loss: 0.4236 - acc: 0.8995 - val_loss: 0.4049 - val_acc: 0.8996\n",
      "Epoch 9/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.3871 - acc: 0.8995 - val_loss: 0.3735 - val_acc: 0.8996\n",
      "Epoch 10/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.3584 - acc: 0.8995 - val_loss: 0.3495 - val_acc: 0.8996\n",
      "Epoch 11/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.3371 - acc: 0.8995 - val_loss: 0.3309 - val_acc: 0.8996\n",
      "Epoch 12/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.3201 - acc: 0.8995 - val_loss: 0.3162 - val_acc: 0.8996\n",
      "Epoch 13/125\n",
      "160000/160000 [==============================] - 12s 73us/step - loss: 0.3071 - acc: 0.8995 - val_loss: 0.3047 - val_acc: 0.8996\n",
      "Epoch 14/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2967 - acc: 0.8995 - val_loss: 0.2954 - val_acc: 0.8996\n",
      "Epoch 15/125\n",
      "160000/160000 [==============================] - 12s 73us/step - loss: 0.2886 - acc: 0.8995 - val_loss: 0.2877 - val_acc: 0.8996\n",
      "Epoch 16/125\n",
      "160000/160000 [==============================] - 10s 65us/step - loss: 0.2820 - acc: 0.8995 - val_loss: 0.2816 - val_acc: 0.8996\n",
      "Epoch 17/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2771 - acc: 0.8995 - val_loss: 0.2769 - val_acc: 0.8996\n",
      "Epoch 18/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2727 - acc: 0.8995 - val_loss: 0.2726 - val_acc: 0.8996\n",
      "Epoch 19/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2693 - acc: 0.8995 - val_loss: 0.2697 - val_acc: 0.8996\n",
      "Epoch 20/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2669 - acc: 0.8995 - val_loss: 0.2670 - val_acc: 0.8996\n",
      "Epoch 21/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2646 - acc: 0.8995 - val_loss: 0.2650 - val_acc: 0.8996\n",
      "Epoch 22/125\n",
      "160000/160000 [==============================] - 23s 141us/step - loss: 0.2628 - acc: 0.8995 - val_loss: 0.2633 - val_acc: 0.8996\n",
      "Epoch 23/125\n",
      "160000/160000 [==============================] - 14s 89us/step - loss: 0.2615 - acc: 0.8995 - val_loss: 0.2622 - val_acc: 0.8996\n",
      "Epoch 24/125\n",
      "160000/160000 [==============================] - 15s 96us/step - loss: 0.2606 - acc: 0.8995 - val_loss: 0.2612 - val_acc: 0.8996\n",
      "Epoch 25/125\n",
      "160000/160000 [==============================] - 15s 94us/step - loss: 0.2595 - acc: 0.8995 - val_loss: 0.2603 - val_acc: 0.8996\n",
      "Epoch 26/125\n",
      "160000/160000 [==============================] - 13s 83us/step - loss: 0.2588 - acc: 0.8995 - val_loss: 0.2597 - val_acc: 0.8996\n",
      "Epoch 27/125\n",
      "160000/160000 [==============================] - 12s 78us/step - loss: 0.2584 - acc: 0.8995 - val_loss: 0.2590 - val_acc: 0.8996\n",
      "Epoch 28/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2577 - acc: 0.8995 - val_loss: 0.2585 - val_acc: 0.8996\n",
      "Epoch 29/125\n",
      "160000/160000 [==============================] - 13s 78us/step - loss: 0.2575 - acc: 0.8995 - val_loss: 0.2582 - val_acc: 0.8996\n",
      "Epoch 30/125\n",
      "160000/160000 [==============================] - 11s 72us/step - loss: 0.2566 - acc: 0.8995 - val_loss: 0.2577 - val_acc: 0.8996\n",
      "Epoch 31/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2565 - acc: 0.8995 - val_loss: 0.2575 - val_acc: 0.8996\n",
      "Epoch 32/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2564 - acc: 0.8995 - val_loss: 0.2572 - val_acc: 0.8996\n",
      "Epoch 33/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2558 - acc: 0.8995 - val_loss: 0.2570 - val_acc: 0.8996\n",
      "Epoch 34/125\n",
      "160000/160000 [==============================] - 10s 60us/step - loss: 0.2558 - acc: 0.8995 - val_loss: 0.2567 - val_acc: 0.8996\n",
      "Epoch 35/125\n",
      "160000/160000 [==============================] - 12s 74us/step - loss: 0.2554 - acc: 0.8995 - val_loss: 0.2564 - val_acc: 0.8996\n",
      "Epoch 36/125\n",
      "160000/160000 [==============================] - 13s 83us/step - loss: 0.2555 - acc: 0.8995 - val_loss: 0.2563 - val_acc: 0.8996\n",
      "Epoch 37/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2550 - acc: 0.8995 - val_loss: 0.2559 - val_acc: 0.8996\n",
      "Epoch 38/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2550 - acc: 0.8995 - val_loss: 0.2559 - val_acc: 0.8996\n",
      "Epoch 39/125\n",
      "160000/160000 [==============================] - 10s 65us/step - loss: 0.2543 - acc: 0.8995 - val_loss: 0.2556 - val_acc: 0.8996\n",
      "Epoch 40/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2544 - acc: 0.8995 - val_loss: 0.2557 - val_acc: 0.8996\n",
      "Epoch 41/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2543 - acc: 0.9062 - val_loss: 0.2555 - val_acc: 0.9122\n",
      "Epoch 42/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2540 - acc: 0.9107 - val_loss: 0.2552 - val_acc: 0.9122\n",
      "Epoch 43/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2540 - acc: 0.9115 - val_loss: 0.2552 - val_acc: 0.9122\n",
      "Epoch 44/125\n",
      "160000/160000 [==============================] - 12s 72us/step - loss: 0.2537 - acc: 0.9115 - val_loss: 0.2550 - val_acc: 0.9124\n",
      "Epoch 45/125\n",
      "160000/160000 [==============================] - 10s 64us/step - loss: 0.2537 - acc: 0.9120 - val_loss: 0.2549 - val_acc: 0.9118\n",
      "Epoch 46/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2537 - acc: 0.9119 - val_loss: 0.2547 - val_acc: 0.9120\n",
      "Epoch 47/125\n",
      "160000/160000 [==============================] - 13s 80us/step - loss: 0.2536 - acc: 0.9117 - val_loss: 0.2546 - val_acc: 0.9129\n",
      "Epoch 48/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2531 - acc: 0.9121 - val_loss: 0.2544 - val_acc: 0.9127\n",
      "Epoch 49/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2532 - acc: 0.9126 - val_loss: 0.2544 - val_acc: 0.9128\n",
      "Epoch 50/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2532 - acc: 0.9122 - val_loss: 0.2541 - val_acc: 0.9133\n",
      "Epoch 51/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2530 - acc: 0.9127 - val_loss: 0.2544 - val_acc: 0.9124\n",
      "Epoch 52/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2529 - acc: 0.9127 - val_loss: 0.2540 - val_acc: 0.9130\n",
      "Epoch 53/125\n",
      "160000/160000 [==============================] - 11s 67us/step - loss: 0.2527 - acc: 0.9122 - val_loss: 0.2539 - val_acc: 0.9124\n",
      "Epoch 54/125\n",
      "160000/160000 [==============================] - 11s 67us/step - loss: 0.2529 - acc: 0.9126 - val_loss: 0.2538 - val_acc: 0.9133\n",
      "Epoch 55/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2526 - acc: 0.9129 - val_loss: 0.2539 - val_acc: 0.9127\n",
      "Epoch 56/125\n",
      "160000/160000 [==============================] - 11s 67us/step - loss: 0.2524 - acc: 0.9122 - val_loss: 0.2536 - val_acc: 0.9126\n",
      "Epoch 57/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2526 - acc: 0.9130 - val_loss: 0.2536 - val_acc: 0.9131\n",
      "Epoch 58/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2522 - acc: 0.9126 - val_loss: 0.2535 - val_acc: 0.9128\n",
      "Epoch 59/125\n",
      "160000/160000 [==============================] - 11s 71us/step - loss: 0.2522 - acc: 0.9126 - val_loss: 0.2537 - val_acc: 0.9124\n",
      "Epoch 60/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2523 - acc: 0.9128 - val_loss: 0.2534 - val_acc: 0.9130\n",
      "Epoch 61/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2518 - acc: 0.9127 - val_loss: 0.2530 - val_acc: 0.9128\n",
      "Epoch 62/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2521 - acc: 0.9128 - val_loss: 0.2532 - val_acc: 0.9128\n",
      "Epoch 63/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2519 - acc: 0.9132 - val_loss: 0.2532 - val_acc: 0.9130\n",
      "Epoch 64/125\n",
      "160000/160000 [==============================] - 11s 67us/step - loss: 0.2521 - acc: 0.9128 - val_loss: 0.2533 - val_acc: 0.9130\n",
      "Epoch 65/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2519 - acc: 0.9129 - val_loss: 0.2530 - val_acc: 0.9129\n",
      "Epoch 66/125\n",
      "160000/160000 [==============================] - 11s 67us/step - loss: 0.2519 - acc: 0.9129 - val_loss: 0.2527 - val_acc: 0.9127\n",
      "Epoch 67/125\n",
      "160000/160000 [==============================] - 12s 73us/step - loss: 0.2515 - acc: 0.9133 - val_loss: 0.2532 - val_acc: 0.9129\n",
      "Epoch 68/125\n",
      "160000/160000 [==============================] - 13s 79us/step - loss: 0.2517 - acc: 0.9128 - val_loss: 0.2527 - val_acc: 0.9131\n",
      "Epoch 69/125\n",
      "160000/160000 [==============================] - 10s 65us/step - loss: 0.2515 - acc: 0.9131 - val_loss: 0.2525 - val_acc: 0.9131\n",
      "Epoch 70/125\n",
      "160000/160000 [==============================] - 11s 67us/step - loss: 0.2515 - acc: 0.9129 - val_loss: 0.2524 - val_acc: 0.9129\n",
      "Epoch 71/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2514 - acc: 0.9134 - val_loss: 0.2529 - val_acc: 0.9128\n",
      "Epoch 72/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2517 - acc: 0.9124 - val_loss: 0.2526 - val_acc: 0.9133\n",
      "Epoch 73/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2512 - acc: 0.9134 - val_loss: 0.2521 - val_acc: 0.9132\n",
      "Epoch 74/125\n",
      "160000/160000 [==============================] - 12s 74us/step - loss: 0.2509 - acc: 0.9131 - val_loss: 0.2527 - val_acc: 0.9130\n",
      "Epoch 75/125\n",
      "160000/160000 [==============================] - 10s 65us/step - loss: 0.2512 - acc: 0.9132 - val_loss: 0.2520 - val_acc: 0.9134\n",
      "Epoch 76/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2509 - acc: 0.9135 - val_loss: 0.2523 - val_acc: 0.9125\n",
      "Epoch 77/125\n",
      "160000/160000 [==============================] - 13s 79us/step - loss: 0.2509 - acc: 0.9129 - val_loss: 0.2520 - val_acc: 0.9136\n",
      "Epoch 78/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2510 - acc: 0.9138 - val_loss: 0.2520 - val_acc: 0.9126\n",
      "Epoch 79/125\n",
      "160000/160000 [==============================] - 13s 82us/step - loss: 0.2507 - acc: 0.9130 - val_loss: 0.2520 - val_acc: 0.9133\n",
      "Epoch 80/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2509 - acc: 0.9134 - val_loss: 0.2517 - val_acc: 0.9135\n",
      "Epoch 81/125\n",
      "160000/160000 [==============================] - 12s 74us/step - loss: 0.2506 - acc: 0.9132 - val_loss: 0.2523 - val_acc: 0.9132\n",
      "Epoch 82/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2511 - acc: 0.9134 - val_loss: 0.2518 - val_acc: 0.9130\n",
      "Epoch 83/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2509 - acc: 0.9129 - val_loss: 0.2523 - val_acc: 0.9137\n",
      "Epoch 84/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2504 - acc: 0.9139 - val_loss: 0.2517 - val_acc: 0.9126\n",
      "Epoch 85/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2505 - acc: 0.9131 - val_loss: 0.2518 - val_acc: 0.9139\n",
      "Epoch 86/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2508 - acc: 0.9134 - val_loss: 0.2522 - val_acc: 0.9128\n",
      "Epoch 87/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2505 - acc: 0.9137 - val_loss: 0.2519 - val_acc: 0.9136\n",
      "Epoch 88/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2503 - acc: 0.9132 - val_loss: 0.2522 - val_acc: 0.9135\n",
      "Epoch 89/125\n",
      "160000/160000 [==============================] - 11s 70us/step - loss: 0.2507 - acc: 0.9134 - val_loss: 0.2512 - val_acc: 0.9131\n",
      "Epoch 90/125\n",
      "160000/160000 [==============================] - 10s 62us/step - loss: 0.2506 - acc: 0.9131 - val_loss: 0.2514 - val_acc: 0.9138\n",
      "Epoch 91/125\n",
      "160000/160000 [==============================] - 12s 74us/step - loss: 0.2508 - acc: 0.9138 - val_loss: 0.2515 - val_acc: 0.9127\n",
      "Epoch 92/125\n",
      "160000/160000 [==============================] - 12s 76us/step - loss: 0.2502 - acc: 0.9133 - val_loss: 0.2514 - val_acc: 0.9137\n",
      "Epoch 93/125\n",
      "160000/160000 [==============================] - 13s 80us/step - loss: 0.2499 - acc: 0.9133 - val_loss: 0.2509 - val_acc: 0.9132\n",
      "Epoch 94/125\n",
      "160000/160000 [==============================] - 12s 78us/step - loss: 0.2500 - acc: 0.9136 - val_loss: 0.2511 - val_acc: 0.9132\n",
      "Epoch 95/125\n",
      "160000/160000 [==============================] - 12s 78us/step - loss: 0.2500 - acc: 0.9134 - val_loss: 0.2513 - val_acc: 0.9137\n",
      "Epoch 96/125\n",
      "160000/160000 [==============================] - 13s 81us/step - loss: 0.2499 - acc: 0.9137 - val_loss: 0.2514 - val_acc: 0.9130\n",
      "Epoch 97/125\n",
      "160000/160000 [==============================] - 12s 76us/step - loss: 0.2500 - acc: 0.9137 - val_loss: 0.2512 - val_acc: 0.9131\n",
      "Epoch 98/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2498 - acc: 0.9132 - val_loss: 0.2507 - val_acc: 0.9136\n",
      "Epoch 99/125\n",
      "160000/160000 [==============================] - 12s 73us/step - loss: 0.2496 - acc: 0.9133 - val_loss: 0.2511 - val_acc: 0.9128\n",
      "Epoch 100/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2499 - acc: 0.9136 - val_loss: 0.2505 - val_acc: 0.9136\n",
      "Epoch 101/125\n",
      "160000/160000 [==============================] - 10s 65us/step - loss: 0.2496 - acc: 0.9135 - val_loss: 0.2508 - val_acc: 0.9137\n",
      "Epoch 102/125\n",
      "160000/160000 [==============================] - 10s 66us/step - loss: 0.2492 - acc: 0.9135 - val_loss: 0.2506 - val_acc: 0.9132\n",
      "Epoch 103/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2497 - acc: 0.9136 - val_loss: 0.2504 - val_acc: 0.9135\n",
      "Epoch 104/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2496 - acc: 0.9134 - val_loss: 0.2506 - val_acc: 0.9129\n",
      "Epoch 105/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2496 - acc: 0.9138 - val_loss: 0.2505 - val_acc: 0.9134\n",
      "Epoch 106/125\n",
      "160000/160000 [==============================] - 10s 64us/step - loss: 0.2495 - acc: 0.9132 - val_loss: 0.2510 - val_acc: 0.9134\n",
      "Epoch 107/125\n",
      "160000/160000 [==============================] - 10s 65us/step - loss: 0.2492 - acc: 0.9135 - val_loss: 0.2505 - val_acc: 0.9135\n",
      "Epoch 108/125\n",
      "160000/160000 [==============================] - 11s 67us/step - loss: 0.2493 - acc: 0.9135 - val_loss: 0.2505 - val_acc: 0.9136\n",
      "Epoch 109/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2495 - acc: 0.9138 - val_loss: 0.2504 - val_acc: 0.9135\n",
      "Epoch 110/125\n",
      "160000/160000 [==============================] - 18s 113us/step - loss: 0.2491 - acc: 0.9137 - val_loss: 0.2501 - val_acc: 0.9135\n",
      "Epoch 111/125\n",
      "160000/160000 [==============================] - 13s 83us/step - loss: 0.2492 - acc: 0.9136 - val_loss: 0.2503 - val_acc: 0.9137\n",
      "Epoch 112/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2492 - acc: 0.9136 - val_loss: 0.2499 - val_acc: 0.9137\n",
      "Epoch 113/125\n",
      "160000/160000 [==============================] - 11s 67us/step - loss: 0.2490 - acc: 0.9136 - val_loss: 0.2505 - val_acc: 0.9129\n",
      "Epoch 114/125\n",
      "160000/160000 [==============================] - 12s 73us/step - loss: 0.2490 - acc: 0.9138 - val_loss: 0.2503 - val_acc: 0.9138\n",
      "Epoch 115/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000/160000 [==============================] - 11s 71us/step - loss: 0.2491 - acc: 0.9139 - val_loss: 0.2500 - val_acc: 0.9134\n",
      "Epoch 116/125\n",
      "160000/160000 [==============================] - 10s 62us/step - loss: 0.2491 - acc: 0.9134 - val_loss: 0.2500 - val_acc: 0.9136\n",
      "Epoch 117/125\n",
      "160000/160000 [==============================] - 10s 63us/step - loss: 0.2491 - acc: 0.9136 - val_loss: 0.2503 - val_acc: 0.9132\n",
      "Epoch 118/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2492 - acc: 0.9140 - val_loss: 0.2505 - val_acc: 0.9137\n",
      "Epoch 119/125\n",
      "160000/160000 [==============================] - 13s 82us/step - loss: 0.2495 - acc: 0.9136 - val_loss: 0.2502 - val_acc: 0.9133\n",
      "Epoch 120/125\n",
      "160000/160000 [==============================] - 14s 85us/step - loss: 0.2492 - acc: 0.9137 - val_loss: 0.2500 - val_acc: 0.9129\n",
      "Epoch 121/125\n",
      "160000/160000 [==============================] - 11s 66us/step - loss: 0.2487 - acc: 0.9136 - val_loss: 0.2499 - val_acc: 0.9134\n",
      "Epoch 122/125\n",
      "160000/160000 [==============================] - 11s 69us/step - loss: 0.2487 - acc: 0.9136 - val_loss: 0.2502 - val_acc: 0.9131\n",
      "Epoch 123/125\n",
      "160000/160000 [==============================] - 13s 84us/step - loss: 0.2486 - acc: 0.9141 - val_loss: 0.2496 - val_acc: 0.9136\n",
      "Epoch 124/125\n",
      "160000/160000 [==============================] - 11s 68us/step - loss: 0.2484 - acc: 0.9137 - val_loss: 0.2496 - val_acc: 0.9135\n",
      "Epoch 125/125\n",
      "160000/160000 [==============================] - 13s 84us/step - loss: 0.2485 - acc: 0.9138 - val_loss: 0.2497 - val_acc: 0.9136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e0280b5c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, batch_size = 16384, epochs = 125, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02324648]\n",
      " [0.00258362]\n",
      " [0.53582466]\n",
      " ...\n",
      " [0.05960162]\n",
      " [0.16806525]\n",
      " [0.01064381]]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_code_test = test['ID_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(featurestotest)\n",
    "out = prediction[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1729722 , 0.21731076, 0.05138029, ..., 0.00252141, 0.05494951,\n",
       "       0.1330659 ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
